{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines import BDQ, DQN, DDPG\n",
    "from stable_baselines.bdq.policies import ActionBranching\n",
    "from stable_baselines.ddpg.policies import MlpPolicy as DDPGMlp\n",
    "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, disc_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(env.reset())[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(DDPGMlp, env, verbose=1, param_noise=param_noise, action_noise=action_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/tf_util.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/tf_util.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:620: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:621: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:518: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/policies.py:174: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "actor q_function [<tf.Tensor 'bdq/model/add:0' shape=(?, 33) dtype=float32>]\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:539: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:638: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:638: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:638: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "q_function to optimize [<tf.Tensor 'bdq/step_model/model/add:0' shape=(?, 33) dtype=float32>]\n",
      "qfunction parameters [<tf.Variable 'bdq/model/common_net/fully_connected/weights:0' shape=(3, 512) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "target q_func [<tf.Tensor 'bdq/target_q_func/model/add:0' shape=(?, 33) dtype=float32>]\n",
      "target q_func vars [<tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/weights:0' shape=(3, 512) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "double q_func [<tf.Tensor 'bdq/q_func/model/add:0' shape=(?, 33) dtype=float32>]\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:713: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:761: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:768: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/build_graph.py:791: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "trainable vars [<tf.Variable 'bdq/eps:0' shape=() dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected/weights:0' shape=(3, 512) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/weights:0' shape=(3, 512) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/tf_util.py:108: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/tf_util.py:109: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BDQ(ActionBranching, env, verbose=2, tensorboard_log='tensorboard_logs/Pendulum', full_tensorboard_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.step_model.q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.processed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/base_class.py:1082: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/common/tf_util.py:188: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/a2c/utils.py:581: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "ep number 1\n",
      "ep number 2\n",
      "ep number 3\n",
      "ep number 4\n",
      "ep number 5\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/bdq.py:315: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/bdq_sb/stable_baselines/bdq/bdq.py:316: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "ep number 6\n",
      "ep number 7\n",
      "ep number 8\n",
      "ep number 9\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 83        |\n",
      "| episodes                | 10        |\n",
      "| mean 100 episode reward | -1.16e+03 |\n",
      "| steps                   | 1799      |\n",
      "---------------------------------------\n",
      "ep number 10\n",
      "ep number 11\n",
      "ep number 12\n",
      "ep number 13\n",
      "ep number 14\n",
      "ep number 15\n",
      "ep number 16\n",
      "ep number 17\n",
      "ep number 18\n",
      "ep number 19\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 65        |\n",
      "| episodes                | 20        |\n",
      "| mean 100 episode reward | -1.29e+03 |\n",
      "| steps                   | 3799      |\n",
      "---------------------------------------\n",
      "ep number 20\n",
      "ep number 21\n",
      "ep number 22\n",
      "ep number 23\n",
      "ep number 24\n",
      "ep number 25\n",
      "ep number 26\n",
      "ep number 27\n",
      "ep number 28\n",
      "ep number 29\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | -1.2e+03 |\n",
      "| steps                   | 5799     |\n",
      "--------------------------------------\n",
      "ep number 30\n",
      "ep number 31\n",
      "ep number 32\n",
      "ep number 33\n",
      "ep number 34\n",
      "ep number 35\n",
      "ep number 36\n",
      "ep number 37\n",
      "ep number 38\n",
      "ep number 39\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 29        |\n",
      "| episodes                | 40        |\n",
      "| mean 100 episode reward | -1.07e+03 |\n",
      "| steps                   | 7799      |\n",
      "---------------------------------------\n",
      "ep number 40\n",
      "ep number 41\n",
      "ep number 42\n",
      "ep number 43\n",
      "ep number 44\n",
      "ep number 45\n",
      "ep number 46\n",
      "ep number 47\n",
      "ep number 48\n",
      "ep number 49\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 11       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | -910     |\n",
      "| steps                   | 9799     |\n",
      "--------------------------------------\n",
      "ep number 50\n",
      "ep number 51\n",
      "ep number 52\n",
      "ep number 53\n",
      "ep number 54\n",
      "ep number 55\n",
      "ep number 56\n",
      "ep number 57\n",
      "ep number 58\n",
      "ep number 59\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | -777     |\n",
      "| steps                   | 11799    |\n",
      "--------------------------------------\n",
      "ep number 60\n",
      "ep number 61\n",
      "ep number 62\n",
      "ep number 63\n",
      "ep number 64\n",
      "ep number 65\n",
      "ep number 66\n",
      "ep number 67\n",
      "ep number 68\n",
      "ep number 69\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | -687     |\n",
      "| steps                   | 13799    |\n",
      "--------------------------------------\n",
      "ep number 70\n",
      "ep number 71\n",
      "ep number 72\n",
      "ep number 73\n",
      "ep number 74\n",
      "ep number 75\n",
      "ep number 76\n",
      "ep number 77\n",
      "ep number 78\n",
      "ep number 79\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | -629     |\n",
      "| steps                   | 15799    |\n",
      "--------------------------------------\n",
      "ep number 80\n",
      "ep number 81\n",
      "ep number 82\n",
      "ep number 83\n",
      "ep number 84\n",
      "ep number 85\n",
      "ep number 86\n",
      "ep number 87\n",
      "ep number 88\n",
      "ep number 89\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | -572     |\n",
      "| steps                   | 17799    |\n",
      "--------------------------------------\n",
      "ep number 90\n",
      "ep number 91\n",
      "ep number 92\n",
      "ep number 93\n",
      "ep number 94\n",
      "ep number 95\n",
      "ep number 96\n",
      "ep number 97\n",
      "ep number 98\n",
      "ep number 99\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -530     |\n",
      "| steps                   | 19799    |\n",
      "--------------------------------------\n",
      "ep number 100\n",
      "ep number 101\n",
      "ep number 102\n",
      "ep number 103\n",
      "ep number 104\n",
      "ep number 105\n",
      "ep number 106\n",
      "ep number 107\n",
      "ep number 108\n",
      "ep number 109\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | -446     |\n",
      "| steps                   | 21799    |\n",
      "--------------------------------------\n",
      "ep number 110\n",
      "ep number 111\n",
      "ep number 112\n",
      "ep number 113\n",
      "ep number 114\n",
      "ep number 115\n",
      "ep number 116\n",
      "ep number 117\n",
      "ep number 118\n",
      "ep number 119\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | -328     |\n",
      "| steps                   | 23799    |\n",
      "--------------------------------------\n",
      "ep number 120\n",
      "ep number 121\n",
      "ep number 122\n",
      "ep number 123\n",
      "ep number 124\n",
      "ep number 125\n",
      "ep number 126\n",
      "ep number 127\n",
      "ep number 128\n",
      "ep number 129\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | -247     |\n",
      "| steps                   | 25799    |\n",
      "--------------------------------------\n",
      "ep number 130\n",
      "ep number 131\n",
      "ep number 132\n",
      "ep number 133\n",
      "ep number 134\n",
      "ep number 135\n",
      "ep number 136\n",
      "ep number 137\n",
      "ep number 138\n",
      "ep number 139\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | -207     |\n",
      "| steps                   | 27799    |\n",
      "--------------------------------------\n",
      "ep number 140\n",
      "ep number 141\n",
      "ep number 142\n",
      "ep number 143\n",
      "ep number 144\n",
      "ep number 145\n",
      "ep number 146\n",
      "ep number 147\n",
      "ep number 148\n",
      "ep number 149\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | -213     |\n",
      "| steps                   | 29799    |\n",
      "--------------------------------------\n",
      "ep number 150\n",
      "ep number 151\n",
      "ep number 152\n",
      "ep number 153\n",
      "ep number 154\n",
      "ep number 155\n",
      "ep number 156\n",
      "ep number 157\n",
      "ep number 158\n",
      "ep number 159\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 7        |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | -230     |\n",
      "| steps                   | 31799    |\n",
      "--------------------------------------\n",
      "ep number 160\n",
      "ep number 161\n",
      "ep number 162\n",
      "ep number 163\n",
      "ep number 164\n",
      "ep number 165\n",
      "ep number 166\n",
      "ep number 167\n",
      "ep number 168\n",
      "ep number 169\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 7        |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | -240     |\n",
      "| steps                   | 33799    |\n",
      "--------------------------------------\n",
      "ep number 170\n",
      "ep number 171\n",
      "ep number 172\n",
      "ep number 173\n",
      "ep number 174\n",
      "ep number 175\n",
      "ep number 176\n",
      "ep number 177\n",
      "ep number 178\n",
      "ep number 179\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 7        |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | -247     |\n",
      "| steps                   | 35799    |\n",
      "--------------------------------------\n",
      "ep number 180\n",
      "ep number 181\n",
      "ep number 182\n",
      "ep number 183\n",
      "ep number 184\n",
      "ep number 185\n",
      "ep number 186\n",
      "ep number 187\n",
      "ep number 188\n",
      "ep number 189\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 7        |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | -262     |\n",
      "| steps                   | 37799    |\n",
      "--------------------------------------\n",
      "ep number 190\n",
      "ep number 191\n",
      "ep number 192\n",
      "ep number 193\n",
      "ep number 194\n",
      "ep number 195\n",
      "ep number 196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep number 197\n",
      "ep number 198\n",
      "ep number 199\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 7        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -277     |\n",
      "| steps                   | 39799    |\n",
      "--------------------------------------\n",
      "ep number 200\n",
      "ep number 201\n",
      "ep number 202\n",
      "ep number 203\n",
      "ep number 204\n",
      "ep number 205\n",
      "ep number 206\n",
      "ep number 207\n",
      "ep number 208\n",
      "ep number 209\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | -276     |\n",
      "| steps                   | 41799    |\n",
      "--------------------------------------\n",
      "ep number 210\n",
      "ep number 211\n",
      "ep number 212\n",
      "ep number 213\n",
      "ep number 214\n",
      "ep number 215\n",
      "ep number 216\n",
      "ep number 217\n",
      "ep number 218\n",
      "ep number 219\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | -284     |\n",
      "| steps                   | 43799    |\n",
      "--------------------------------------\n",
      "ep number 220\n",
      "ep number 221\n",
      "ep number 222\n",
      "ep number 223\n",
      "ep number 224\n",
      "ep number 225\n",
      "ep number 226\n",
      "ep number 227\n",
      "ep number 228\n",
      "ep number 229\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | -291     |\n",
      "| steps                   | 45799    |\n",
      "--------------------------------------\n",
      "ep number 230\n",
      "ep number 231\n",
      "ep number 232\n",
      "ep number 233\n",
      "ep number 234\n",
      "ep number 235\n",
      "ep number 236\n",
      "ep number 237\n",
      "ep number 238\n",
      "ep number 239\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | -290     |\n",
      "| steps                   | 47799    |\n",
      "--------------------------------------\n",
      "ep number 240\n",
      "ep number 241\n",
      "ep number 242\n",
      "ep number 243\n",
      "ep number 244\n",
      "ep number 245\n",
      "ep number 246\n",
      "ep number 247\n",
      "ep number 248\n",
      "ep number 249\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | -288     |\n",
      "| steps                   | 49799    |\n",
      "--------------------------------------\n",
      "ep number 250\n",
      "ep number 251\n",
      "ep number 252\n",
      "ep number 253\n",
      "ep number 254\n",
      "ep number 255\n",
      "ep number 256\n",
      "ep number 257\n",
      "ep number 258\n",
      "ep number 259\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 260      |\n",
      "| mean 100 episode reward | -297     |\n",
      "| steps                   | 51799    |\n",
      "--------------------------------------\n",
      "ep number 260\n",
      "ep number 261\n",
      "ep number 262\n",
      "ep number 263\n",
      "ep number 264\n",
      "ep number 265\n",
      "ep number 266\n",
      "ep number 267\n",
      "ep number 268\n",
      "ep number 269\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 270      |\n",
      "| mean 100 episode reward | -302     |\n",
      "| steps                   | 53799    |\n",
      "--------------------------------------\n",
      "ep number 270\n",
      "ep number 271\n",
      "ep number 272\n",
      "ep number 273\n",
      "ep number 274\n",
      "ep number 275\n",
      "ep number 276\n",
      "ep number 277\n",
      "ep number 278\n",
      "ep number 279\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 280      |\n",
      "| mean 100 episode reward | -298     |\n",
      "| steps                   | 55799    |\n",
      "--------------------------------------\n",
      "ep number 280\n",
      "ep number 281\n",
      "ep number 282\n",
      "ep number 283\n",
      "ep number 284\n",
      "ep number 285\n",
      "ep number 286\n",
      "ep number 287\n",
      "ep number 288\n",
      "ep number 289\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 290      |\n",
      "| mean 100 episode reward | -282     |\n",
      "| steps                   | 57799    |\n",
      "--------------------------------------\n",
      "ep number 290\n",
      "ep number 291\n",
      "ep number 292\n",
      "ep number 293\n",
      "ep number 294\n",
      "ep number 295\n",
      "ep number 296\n",
      "ep number 297\n",
      "ep number 298\n",
      "ep number 299\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 5        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -273     |\n",
      "| steps                   | 59799    |\n",
      "--------------------------------------\n",
      "ep number 300\n",
      "ep number 301\n",
      "ep number 302\n",
      "ep number 303\n",
      "ep number 304\n",
      "ep number 305\n",
      "ep number 306\n",
      "ep number 307\n",
      "ep number 308\n",
      "ep number 309\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 310      |\n",
      "| mean 100 episode reward | -258     |\n",
      "| steps                   | 61799    |\n",
      "--------------------------------------\n",
      "ep number 310\n",
      "ep number 311\n",
      "ep number 312\n",
      "ep number 313\n",
      "ep number 314\n",
      "ep number 315\n",
      "ep number 316\n",
      "ep number 317\n",
      "ep number 318\n",
      "ep number 319\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 320      |\n",
      "| mean 100 episode reward | -246     |\n",
      "| steps                   | 63799    |\n",
      "--------------------------------------\n",
      "ep number 320\n",
      "ep number 321\n",
      "ep number 322\n",
      "ep number 323\n",
      "ep number 324\n",
      "ep number 325\n",
      "ep number 326\n",
      "ep number 327\n",
      "ep number 328\n",
      "ep number 329\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 330      |\n",
      "| mean 100 episode reward | -234     |\n",
      "| steps                   | 65799    |\n",
      "--------------------------------------\n",
      "ep number 330\n",
      "ep number 331\n",
      "ep number 332\n",
      "ep number 333\n",
      "ep number 334\n",
      "ep number 335\n",
      "ep number 336\n",
      "ep number 337\n",
      "ep number 338\n",
      "ep number 339\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 340      |\n",
      "| mean 100 episode reward | -219     |\n",
      "| steps                   | 67799    |\n",
      "--------------------------------------\n",
      "ep number 340\n",
      "ep number 341\n",
      "ep number 342\n",
      "ep number 343\n",
      "ep number 344\n",
      "ep number 345\n",
      "ep number 346\n",
      "ep number 347\n",
      "ep number 348\n",
      "ep number 349\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | -209     |\n",
      "| steps                   | 69799    |\n",
      "--------------------------------------\n",
      "ep number 350\n",
      "ep number 351\n",
      "ep number 352\n",
      "ep number 353\n",
      "ep number 354\n",
      "ep number 355\n",
      "ep number 356\n",
      "ep number 357\n",
      "ep number 358\n",
      "ep number 359\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 360      |\n",
      "| mean 100 episode reward | -190     |\n",
      "| steps                   | 71799    |\n",
      "--------------------------------------\n",
      "ep number 360\n",
      "ep number 361\n",
      "ep number 362\n",
      "ep number 363\n",
      "ep number 364\n",
      "ep number 365\n",
      "ep number 366\n",
      "ep number 367\n",
      "ep number 368\n",
      "ep number 369\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 370      |\n",
      "| mean 100 episode reward | -178     |\n",
      "| steps                   | 73799    |\n",
      "--------------------------------------\n",
      "ep number 370\n",
      "ep number 371\n",
      "ep number 372\n",
      "ep number 373\n",
      "ep number 374\n",
      "ep number 375\n",
      "ep number 376\n",
      "ep number 377\n",
      "ep number 378\n",
      "ep number 379\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 380      |\n",
      "| mean 100 episode reward | -179     |\n",
      "| steps                   | 75799    |\n",
      "--------------------------------------\n",
      "ep number 380\n",
      "ep number 381\n",
      "ep number 382\n",
      "ep number 383\n",
      "ep number 384\n",
      "ep number 385\n",
      "ep number 386\n",
      "ep number 387\n",
      "ep number 388\n",
      "ep number 389\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 390      |\n",
      "| mean 100 episode reward | -197     |\n",
      "| steps                   | 77799    |\n",
      "--------------------------------------\n",
      "ep number 390\n",
      "ep number 391\n",
      "ep number 392\n",
      "ep number 393\n",
      "ep number 394\n",
      "ep number 395\n",
      "ep number 396\n",
      "ep number 397\n",
      "ep number 398\n",
      "ep number 399\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -201     |\n",
      "| steps                   | 79799    |\n",
      "--------------------------------------\n",
      "ep number 400\n",
      "ep number 401\n",
      "ep number 402\n",
      "ep number 403\n",
      "ep number 404\n",
      "ep number 405\n",
      "ep number 406\n",
      "ep number 407\n",
      "ep number 408\n",
      "ep number 409\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 410      |\n",
      "| mean 100 episode reward | -207     |\n",
      "| steps                   | 81799    |\n",
      "--------------------------------------\n",
      "ep number 410\n",
      "ep number 411\n",
      "ep number 412\n",
      "ep number 413\n",
      "ep number 414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep number 415\n",
      "ep number 416\n",
      "ep number 417\n",
      "ep number 418\n",
      "ep number 419\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 420      |\n",
      "| mean 100 episode reward | -208     |\n",
      "| steps                   | 83799    |\n",
      "--------------------------------------\n",
      "ep number 420\n",
      "ep number 421\n",
      "ep number 422\n",
      "ep number 423\n",
      "ep number 424\n",
      "ep number 425\n",
      "ep number 426\n",
      "ep number 427\n",
      "ep number 428\n",
      "ep number 429\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 430      |\n",
      "| mean 100 episode reward | -213     |\n",
      "| steps                   | 85799    |\n",
      "--------------------------------------\n",
      "ep number 430\n",
      "ep number 431\n",
      "ep number 432\n",
      "ep number 433\n",
      "ep number 434\n",
      "ep number 435\n",
      "ep number 436\n",
      "ep number 437\n",
      "ep number 438\n",
      "ep number 439\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 440      |\n",
      "| mean 100 episode reward | -215     |\n",
      "| steps                   | 87799    |\n",
      "--------------------------------------\n",
      "ep number 440\n",
      "ep number 441\n",
      "ep number 442\n",
      "ep number 443\n",
      "ep number 444\n",
      "ep number 445\n",
      "ep number 446\n",
      "ep number 447\n",
      "ep number 448\n",
      "ep number 449\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | -220     |\n",
      "| steps                   | 89799    |\n",
      "--------------------------------------\n",
      "ep number 450\n",
      "ep number 451\n",
      "ep number 452\n",
      "ep number 453\n",
      "ep number 454\n",
      "ep number 455\n",
      "ep number 456\n",
      "ep number 457\n",
      "ep number 458\n",
      "ep number 459\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 460      |\n",
      "| mean 100 episode reward | -219     |\n",
      "| steps                   | 91799    |\n",
      "--------------------------------------\n",
      "ep number 460\n",
      "ep number 461\n",
      "ep number 462\n",
      "ep number 463\n",
      "ep number 464\n",
      "ep number 465\n",
      "ep number 466\n",
      "ep number 467\n",
      "ep number 468\n",
      "ep number 469\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 470      |\n",
      "| mean 100 episode reward | -225     |\n",
      "| steps                   | 93799    |\n",
      "--------------------------------------\n",
      "ep number 470\n",
      "ep number 471\n",
      "ep number 472\n",
      "ep number 473\n",
      "ep number 474\n",
      "ep number 475\n",
      "ep number 476\n",
      "ep number 477\n",
      "ep number 478\n",
      "ep number 479\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 480      |\n",
      "| mean 100 episode reward | -221     |\n",
      "| steps                   | 95799    |\n",
      "--------------------------------------\n",
      "ep number 480\n",
      "ep number 481\n",
      "ep number 482\n",
      "ep number 483\n",
      "ep number 484\n",
      "ep number 485\n",
      "ep number 486\n",
      "ep number 487\n",
      "ep number 488\n",
      "ep number 489\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 490      |\n",
      "| mean 100 episode reward | -234     |\n",
      "| steps                   | 97799    |\n",
      "--------------------------------------\n",
      "ep number 490\n",
      "ep number 491\n",
      "ep number 492\n",
      "ep number 493\n",
      "ep number 494\n",
      "ep number 495\n",
      "ep number 496\n",
      "ep number 497\n",
      "ep number 498\n",
      "ep number 499\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -237     |\n",
      "| steps                   | 99799    |\n",
      "--------------------------------------\n",
      "ep number 500\n",
      "ep number 501\n",
      "ep number 502\n",
      "ep number 503\n",
      "ep number 504\n",
      "ep number 505\n",
      "ep number 506\n",
      "ep number 507\n",
      "ep number 508\n",
      "ep number 509\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 510      |\n",
      "| mean 100 episode reward | -246     |\n",
      "| steps                   | 101799   |\n",
      "--------------------------------------\n",
      "ep number 510\n",
      "ep number 511\n",
      "ep number 512\n",
      "ep number 513\n",
      "ep number 514\n",
      "ep number 515\n",
      "ep number 516\n",
      "ep number 517\n",
      "ep number 518\n",
      "ep number 519\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 520      |\n",
      "| mean 100 episode reward | -245     |\n",
      "| steps                   | 103799   |\n",
      "--------------------------------------\n",
      "ep number 520\n",
      "ep number 521\n",
      "ep number 522\n",
      "ep number 523\n",
      "ep number 524\n",
      "ep number 525\n",
      "ep number 526\n",
      "ep number 527\n",
      "ep number 528\n",
      "ep number 529\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 530      |\n",
      "| mean 100 episode reward | -250     |\n",
      "| steps                   | 105799   |\n",
      "--------------------------------------\n",
      "ep number 530\n",
      "ep number 531\n",
      "ep number 532\n",
      "ep number 533\n",
      "ep number 534\n",
      "ep number 535\n",
      "ep number 536\n",
      "ep number 537\n",
      "ep number 538\n",
      "ep number 539\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 540      |\n",
      "| mean 100 episode reward | -258     |\n",
      "| steps                   | 107799   |\n",
      "--------------------------------------\n",
      "ep number 540\n",
      "ep number 541\n",
      "ep number 542\n",
      "ep number 543\n",
      "ep number 544\n",
      "ep number 545\n",
      "ep number 546\n",
      "ep number 547\n",
      "ep number 548\n",
      "ep number 549\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | -254     |\n",
      "| steps                   | 109799   |\n",
      "--------------------------------------\n",
      "ep number 550\n",
      "ep number 551\n",
      "ep number 552\n",
      "ep number 553\n",
      "ep number 554\n",
      "ep number 555\n",
      "ep number 556\n",
      "ep number 557\n",
      "ep number 558\n",
      "ep number 559\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 560      |\n",
      "| mean 100 episode reward | -261     |\n",
      "| steps                   | 111799   |\n",
      "--------------------------------------\n",
      "ep number 560\n",
      "ep number 561\n",
      "ep number 562\n",
      "ep number 563\n",
      "ep number 564\n",
      "ep number 565\n",
      "ep number 566\n",
      "ep number 567\n",
      "ep number 568\n",
      "ep number 569\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 570      |\n",
      "| mean 100 episode reward | -264     |\n",
      "| steps                   | 113799   |\n",
      "--------------------------------------\n",
      "ep number 570\n",
      "ep number 571\n",
      "ep number 572\n",
      "ep number 573\n",
      "ep number 574\n",
      "ep number 575\n",
      "ep number 576\n",
      "ep number 577\n",
      "ep number 578\n",
      "ep number 579\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 580      |\n",
      "| mean 100 episode reward | -272     |\n",
      "| steps                   | 115799   |\n",
      "--------------------------------------\n",
      "ep number 580\n",
      "ep number 581\n",
      "ep number 582\n",
      "ep number 583\n",
      "ep number 584\n",
      "ep number 585\n",
      "ep number 586\n",
      "ep number 587\n",
      "ep number 588\n",
      "ep number 589\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 590      |\n",
      "| mean 100 episode reward | -262     |\n",
      "| steps                   | 117799   |\n",
      "--------------------------------------\n",
      "ep number 590\n",
      "ep number 591\n",
      "ep number 592\n",
      "ep number 593\n",
      "ep number 594\n",
      "ep number 595\n",
      "ep number 596\n",
      "ep number 597\n",
      "ep number 598\n",
      "ep number 599\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -252     |\n",
      "| steps                   | 119799   |\n",
      "--------------------------------------\n",
      "ep number 600\n",
      "ep number 601\n",
      "ep number 602\n",
      "ep number 603\n",
      "ep number 604\n",
      "ep number 605\n",
      "ep number 606\n",
      "ep number 607\n",
      "ep number 608\n",
      "ep number 609\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 610      |\n",
      "| mean 100 episode reward | -246     |\n",
      "| steps                   | 121799   |\n",
      "--------------------------------------\n",
      "ep number 610\n",
      "ep number 611\n",
      "ep number 612\n",
      "ep number 613\n",
      "ep number 614\n",
      "ep number 615\n",
      "ep number 616\n",
      "ep number 617\n",
      "ep number 618\n",
      "ep number 619\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 620      |\n",
      "| mean 100 episode reward | -246     |\n",
      "| steps                   | 123799   |\n",
      "--------------------------------------\n",
      "ep number 620\n",
      "ep number 621\n",
      "ep number 622\n",
      "ep number 623\n",
      "ep number 624\n",
      "ep number 625\n",
      "ep number 626\n",
      "ep number 627\n",
      "ep number 628\n",
      "ep number 629\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 630      |\n",
      "| mean 100 episode reward | -244     |\n",
      "| steps                   | 125799   |\n",
      "--------------------------------------\n",
      "ep number 630\n",
      "ep number 631\n",
      "ep number 632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep number 633\n",
      "ep number 634\n",
      "ep number 635\n",
      "ep number 636\n",
      "ep number 637\n",
      "ep number 638\n",
      "ep number 639\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 640      |\n",
      "| mean 100 episode reward | -245     |\n",
      "| steps                   | 127799   |\n",
      "--------------------------------------\n",
      "ep number 640\n",
      "ep number 641\n",
      "ep number 642\n",
      "ep number 643\n",
      "ep number 644\n",
      "ep number 645\n",
      "ep number 646\n",
      "ep number 647\n",
      "ep number 648\n",
      "ep number 649\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 650      |\n",
      "| mean 100 episode reward | -244     |\n",
      "| steps                   | 129799   |\n",
      "--------------------------------------\n",
      "ep number 650\n",
      "ep number 651\n",
      "ep number 652\n",
      "ep number 653\n",
      "ep number 654\n",
      "ep number 655\n",
      "ep number 656\n",
      "ep number 657\n",
      "ep number 658\n",
      "ep number 659\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 660      |\n",
      "| mean 100 episode reward | -235     |\n",
      "| steps                   | 131799   |\n",
      "--------------------------------------\n",
      "ep number 660\n",
      "ep number 661\n",
      "ep number 662\n",
      "ep number 663\n",
      "ep number 664\n",
      "ep number 665\n",
      "ep number 666\n",
      "ep number 667\n",
      "ep number 668\n",
      "ep number 669\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 670      |\n",
      "| mean 100 episode reward | -224     |\n",
      "| steps                   | 133799   |\n",
      "--------------------------------------\n",
      "ep number 670\n",
      "ep number 671\n",
      "ep number 672\n",
      "ep number 673\n",
      "ep number 674\n",
      "ep number 675\n",
      "ep number 676\n",
      "ep number 677\n",
      "ep number 678\n",
      "ep number 679\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 680      |\n",
      "| mean 100 episode reward | -220     |\n",
      "| steps                   | 135799   |\n",
      "--------------------------------------\n",
      "ep number 680\n",
      "ep number 681\n",
      "ep number 682\n",
      "ep number 683\n",
      "ep number 684\n",
      "ep number 685\n",
      "ep number 686\n",
      "ep number 687\n",
      "ep number 688\n",
      "ep number 689\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 690      |\n",
      "| mean 100 episode reward | -214     |\n",
      "| steps                   | 137799   |\n",
      "--------------------------------------\n",
      "ep number 690\n",
      "ep number 691\n",
      "ep number 692\n",
      "ep number 693\n",
      "ep number 694\n",
      "ep number 695\n",
      "ep number 696\n",
      "ep number 697\n",
      "ep number 698\n",
      "ep number 699\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -212     |\n",
      "| steps                   | 139799   |\n",
      "--------------------------------------\n",
      "ep number 700\n",
      "ep number 701\n",
      "ep number 702\n",
      "ep number 703\n",
      "ep number 704\n",
      "ep number 705\n",
      "ep number 706\n",
      "ep number 707\n",
      "ep number 708\n",
      "ep number 709\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 710      |\n",
      "| mean 100 episode reward | -213     |\n",
      "| steps                   | 141799   |\n",
      "--------------------------------------\n",
      "ep number 710\n",
      "ep number 711\n",
      "ep number 712\n",
      "ep number 713\n",
      "ep number 714\n",
      "ep number 715\n",
      "ep number 716\n",
      "ep number 717\n",
      "ep number 718\n",
      "ep number 719\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 720      |\n",
      "| mean 100 episode reward | -217     |\n",
      "| steps                   | 143799   |\n",
      "--------------------------------------\n",
      "ep number 720\n",
      "ep number 721\n",
      "ep number 722\n",
      "ep number 723\n",
      "ep number 724\n",
      "ep number 725\n",
      "ep number 726\n",
      "ep number 727\n",
      "ep number 728\n",
      "ep number 729\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 730      |\n",
      "| mean 100 episode reward | -217     |\n",
      "| steps                   | 145799   |\n",
      "--------------------------------------\n",
      "ep number 730\n",
      "ep number 731\n",
      "ep number 732\n",
      "ep number 733\n",
      "ep number 734\n",
      "ep number 735\n",
      "ep number 736\n",
      "ep number 737\n",
      "ep number 738\n",
      "ep number 739\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 740      |\n",
      "| mean 100 episode reward | -222     |\n",
      "| steps                   | 147799   |\n",
      "--------------------------------------\n",
      "ep number 740\n",
      "ep number 741\n",
      "ep number 742\n",
      "ep number 743\n",
      "ep number 744\n",
      "ep number 745\n",
      "ep number 746\n",
      "ep number 747\n",
      "ep number 748\n",
      "ep number 749\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 750      |\n",
      "| mean 100 episode reward | -223     |\n",
      "| steps                   | 149799   |\n",
      "--------------------------------------\n",
      "ep number 750\n",
      "ep number 751\n",
      "ep number 752\n",
      "ep number 753\n",
      "ep number 754\n",
      "ep number 755\n",
      "ep number 756\n",
      "ep number 757\n",
      "ep number 758\n",
      "ep number 759\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 760      |\n",
      "| mean 100 episode reward | -222     |\n",
      "| steps                   | 151799   |\n",
      "--------------------------------------\n",
      "ep number 760\n",
      "ep number 761\n",
      "ep number 762\n",
      "ep number 763\n",
      "ep number 764\n",
      "ep number 765\n",
      "ep number 766\n",
      "ep number 767\n",
      "ep number 768\n",
      "ep number 769\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 770      |\n",
      "| mean 100 episode reward | -228     |\n",
      "| steps                   | 153799   |\n",
      "--------------------------------------\n",
      "ep number 770\n",
      "ep number 771\n",
      "ep number 772\n",
      "ep number 773\n",
      "ep number 774\n",
      "ep number 775\n",
      "ep number 776\n",
      "ep number 777\n",
      "ep number 778\n",
      "ep number 779\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 780      |\n",
      "| mean 100 episode reward | -224     |\n",
      "| steps                   | 155799   |\n",
      "--------------------------------------\n",
      "ep number 780\n",
      "ep number 781\n",
      "ep number 782\n",
      "ep number 783\n",
      "ep number 784\n",
      "ep number 785\n",
      "ep number 786\n",
      "ep number 787\n",
      "ep number 788\n",
      "ep number 789\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 790      |\n",
      "| mean 100 episode reward | -209     |\n",
      "| steps                   | 157799   |\n",
      "--------------------------------------\n",
      "ep number 790\n",
      "ep number 791\n",
      "ep number 792\n",
      "ep number 793\n",
      "ep number 794\n",
      "ep number 795\n",
      "ep number 796\n",
      "ep number 797\n",
      "ep number 798\n",
      "ep number 799\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -207     |\n",
      "| steps                   | 159799   |\n",
      "--------------------------------------\n",
      "ep number 800\n",
      "ep number 801\n",
      "ep number 802\n",
      "ep number 803\n",
      "ep number 804\n",
      "ep number 805\n",
      "ep number 806\n",
      "ep number 807\n",
      "ep number 808\n",
      "ep number 809\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 810      |\n",
      "| mean 100 episode reward | -209     |\n",
      "| steps                   | 161799   |\n",
      "--------------------------------------\n",
      "ep number 810\n",
      "ep number 811\n",
      "ep number 812\n",
      "ep number 813\n",
      "ep number 814\n",
      "ep number 815\n",
      "ep number 816\n",
      "ep number 817\n",
      "ep number 818\n",
      "ep number 819\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 820      |\n",
      "| mean 100 episode reward | -203     |\n",
      "| steps                   | 163799   |\n",
      "--------------------------------------\n",
      "ep number 820\n",
      "ep number 821\n",
      "ep number 822\n",
      "ep number 823\n",
      "ep number 824\n",
      "ep number 825\n",
      "ep number 826\n",
      "ep number 827\n",
      "ep number 828\n",
      "ep number 829\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 830      |\n",
      "| mean 100 episode reward | -200     |\n",
      "| steps                   | 165799   |\n",
      "--------------------------------------\n",
      "ep number 830\n",
      "ep number 831\n",
      "ep number 832\n",
      "ep number 833\n",
      "ep number 834\n",
      "ep number 835\n",
      "ep number 836\n",
      "ep number 837\n",
      "ep number 838\n",
      "ep number 839\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 840      |\n",
      "| mean 100 episode reward | -188     |\n",
      "| steps                   | 167799   |\n",
      "--------------------------------------\n",
      "ep number 840\n",
      "ep number 841\n",
      "ep number 842\n",
      "ep number 843\n",
      "ep number 844\n",
      "ep number 845\n",
      "ep number 846\n",
      "ep number 847\n",
      "ep number 848\n",
      "ep number 849\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 850      |\n",
      "| mean 100 episode reward | -183     |\n",
      "| steps                   | 169799   |\n",
      "--------------------------------------\n",
      "ep number 850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep number 851\n",
      "ep number 852\n",
      "ep number 853\n",
      "ep number 854\n",
      "ep number 855\n",
      "ep number 856\n",
      "ep number 857\n",
      "ep number 858\n",
      "ep number 859\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 860      |\n",
      "| mean 100 episode reward | -192     |\n",
      "| steps                   | 171799   |\n",
      "--------------------------------------\n",
      "ep number 860\n",
      "ep number 861\n",
      "ep number 862\n",
      "ep number 863\n",
      "ep number 864\n",
      "ep number 865\n",
      "ep number 866\n",
      "ep number 867\n",
      "ep number 868\n",
      "ep number 869\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 870      |\n",
      "| mean 100 episode reward | -190     |\n",
      "| steps                   | 173799   |\n",
      "--------------------------------------\n",
      "ep number 870\n",
      "ep number 871\n",
      "ep number 872\n",
      "ep number 873\n",
      "ep number 874\n",
      "ep number 875\n",
      "ep number 876\n",
      "ep number 877\n",
      "ep number 878\n",
      "ep number 879\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 880      |\n",
      "| mean 100 episode reward | -185     |\n",
      "| steps                   | 175799   |\n",
      "--------------------------------------\n",
      "ep number 880\n",
      "ep number 881\n",
      "ep number 882\n",
      "ep number 883\n",
      "ep number 884\n",
      "ep number 885\n",
      "ep number 886\n",
      "ep number 887\n",
      "ep number 888\n",
      "ep number 889\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 890      |\n",
      "| mean 100 episode reward | -191     |\n",
      "| steps                   | 177799   |\n",
      "--------------------------------------\n",
      "ep number 890\n",
      "ep number 891\n",
      "ep number 892\n",
      "ep number 893\n",
      "ep number 894\n",
      "ep number 895\n",
      "ep number 896\n",
      "ep number 897\n",
      "ep number 898\n",
      "ep number 899\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -202     |\n",
      "| steps                   | 179799   |\n",
      "--------------------------------------\n",
      "ep number 900\n",
      "ep number 901\n",
      "ep number 902\n",
      "ep number 903\n",
      "ep number 904\n",
      "ep number 905\n",
      "ep number 906\n",
      "ep number 907\n",
      "ep number 908\n",
      "ep number 909\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 910      |\n",
      "| mean 100 episode reward | -204     |\n",
      "| steps                   | 181799   |\n",
      "--------------------------------------\n",
      "ep number 910\n",
      "ep number 911\n",
      "ep number 912\n",
      "ep number 913\n",
      "ep number 914\n",
      "ep number 915\n",
      "ep number 916\n",
      "ep number 917\n",
      "ep number 918\n",
      "ep number 919\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 920      |\n",
      "| mean 100 episode reward | -208     |\n",
      "| steps                   | 183799   |\n",
      "--------------------------------------\n",
      "ep number 920\n",
      "ep number 921\n",
      "ep number 922\n",
      "ep number 923\n",
      "ep number 924\n",
      "ep number 925\n",
      "ep number 926\n",
      "ep number 927\n",
      "ep number 928\n",
      "ep number 929\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 930      |\n",
      "| mean 100 episode reward | -219     |\n",
      "| steps                   | 185799   |\n",
      "--------------------------------------\n",
      "ep number 930\n",
      "ep number 931\n",
      "ep number 932\n",
      "ep number 933\n",
      "ep number 934\n",
      "ep number 935\n",
      "ep number 936\n",
      "ep number 937\n",
      "ep number 938\n",
      "ep number 939\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 940      |\n",
      "| mean 100 episode reward | -238     |\n",
      "| steps                   | 187799   |\n",
      "--------------------------------------\n",
      "ep number 940\n",
      "ep number 941\n",
      "ep number 942\n",
      "ep number 943\n",
      "ep number 944\n",
      "ep number 945\n",
      "ep number 946\n",
      "ep number 947\n",
      "ep number 948\n",
      "ep number 949\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 950      |\n",
      "| mean 100 episode reward | -258     |\n",
      "| steps                   | 189799   |\n",
      "--------------------------------------\n",
      "ep number 950\n",
      "ep number 951\n",
      "ep number 952\n",
      "ep number 953\n",
      "ep number 954\n",
      "ep number 955\n",
      "ep number 956\n",
      "ep number 957\n",
      "ep number 958\n",
      "ep number 959\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 960      |\n",
      "| mean 100 episode reward | -254     |\n",
      "| steps                   | 191799   |\n",
      "--------------------------------------\n",
      "ep number 960\n",
      "ep number 961\n",
      "ep number 962\n",
      "ep number 963\n",
      "ep number 964\n",
      "ep number 965\n",
      "ep number 966\n",
      "ep number 967\n",
      "ep number 968\n",
      "ep number 969\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 970      |\n",
      "| mean 100 episode reward | -253     |\n",
      "| steps                   | 193799   |\n",
      "--------------------------------------\n",
      "ep number 970\n",
      "ep number 971\n",
      "ep number 972\n",
      "ep number 973\n",
      "ep number 974\n",
      "ep number 975\n",
      "ep number 976\n",
      "ep number 977\n",
      "ep number 978\n",
      "ep number 979\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 980      |\n",
      "| mean 100 episode reward | -252     |\n",
      "| steps                   | 195799   |\n",
      "--------------------------------------\n",
      "ep number 980\n",
      "ep number 981\n",
      "ep number 982\n",
      "ep number 983\n",
      "ep number 984\n",
      "ep number 985\n",
      "ep number 986\n",
      "ep number 987\n",
      "ep number 988\n",
      "ep number 989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 990      |\n",
      "| mean 100 episode reward | -259     |\n",
      "| steps                   | 197799   |\n",
      "--------------------------------------\n",
      "ep number 990\n",
      "ep number 991\n",
      "ep number 992\n",
      "ep number 993\n",
      "ep number 994\n",
      "ep number 995\n",
      "ep number 996\n",
      "ep number 997\n",
      "ep number 998\n",
      "ep number 999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | -264     |\n",
      "| steps                   | 199799   |\n",
      "--------------------------------------\n",
      "ep number 1000\n",
      "ep number 1001\n",
      "ep number 1002\n",
      "ep number 1003\n",
      "ep number 1004\n",
      "ep number 1005\n",
      "ep number 1006\n",
      "ep number 1007\n",
      "ep number 1008\n",
      "ep number 1009\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1010     |\n",
      "| mean 100 episode reward | -262     |\n",
      "| steps                   | 201799   |\n",
      "--------------------------------------\n",
      "ep number 1010\n",
      "ep number 1011\n",
      "ep number 1012\n",
      "ep number 1013\n",
      "ep number 1014\n",
      "ep number 1015\n",
      "ep number 1016\n",
      "ep number 1017\n",
      "ep number 1018\n",
      "ep number 1019\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1020     |\n",
      "| mean 100 episode reward | -266     |\n",
      "| steps                   | 203799   |\n",
      "--------------------------------------\n",
      "ep number 1020\n",
      "ep number 1021\n",
      "ep number 1022\n",
      "ep number 1023\n",
      "ep number 1024\n",
      "ep number 1025\n",
      "ep number 1026\n",
      "ep number 1027\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Bdq_mountaincarNew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('Bdq_mountaincar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "#     print(action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "#     print(rewards)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(acts)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_action",
   "language": "python",
   "name": "sb_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
