{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import BDQ, DQN, DDPG\n",
    "from stable_baselines.bdq.policies import ActionBranching\n",
    "from stable_baselines.ddpg.policies import MlpPolicy as DDPGMlp\n",
    "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, disc_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(env.reset())[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(DDPGMlp, env, verbose=1, param_noise=param_noise, action_noise=action_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1468f46a6d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/tf_util.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/tf_util.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/bdq.py:144: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:624: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/policies.py:174: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:206: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:220: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:642: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:642: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:642: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "q_function to optimize <stable_baselines.bdq.policies.ActionBranching object at 0x13fe64e48>\n",
      "qfunction parameters [<tf.Variable 'bdq/model/common_net/fully_connected/weights:0' shape=(11, 512) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>, <tf.Variable 'bdq/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_2/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_3/weights:0' shape=(128, 33) dtype=float32_ref>, <tf.Variable 'bdq/model/action_value/fully_connected_3/biases:0' shape=(33,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'bdq/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "WARNING:tensorflow:From /Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:762: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:766: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/build_graph.py:789: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/tf_util.py:108: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/tf_util.py:109: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BDQ(ActionBranching, env, verbose=2, tensorboard_log='MountainCar', full_tensorboard_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bdq/model/common_net/fully_connected/weights:0' shape=(2, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/eps:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/weights:0' shape=(2, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/weights:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/common_net/fully_connected_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/weights:0' shape=(128, 33) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/action_value/fully_connected_1/biases:0' shape=(33,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'bdq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameter_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'bdq/step_model/model/add:0' shape=(?, 33) dtype=float32>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.step_model.q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.processed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/base_class.py:1082: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Creating window glfw\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/common/tf_util.py:188: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/a2c/utils.py:581: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "ep number 1\n",
      "ep number 2\n",
      "ep number 3\n",
      "ep number 4\n",
      "ep number 5\n",
      "ep number 6\n",
      "ep number 7\n",
      "ep number 8\n",
      "ep number 9\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | -43.8    |\n",
      "| steps                   | 449      |\n",
      "--------------------------------------\n",
      "ep number 10\n",
      "ep number 11\n",
      "ep number 12\n",
      "ep number 13\n",
      "ep number 14\n",
      "ep number 15\n",
      "ep number 16\n",
      "ep number 17\n",
      "ep number 18\n",
      "ep number 19\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | -45.8    |\n",
      "| steps                   | 949      |\n",
      "--------------------------------------\n",
      "ep number 20\n",
      "ep number 21\n",
      "ep number 22\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/bdq.py:287: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/barisyazici/Desktop/ActionBranchingQ-Learning/algorithms/stable-baselines/stable_baselines/bdq/bdq.py:288: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "ep number 23\n",
      "ep number 24\n",
      "ep number 25\n",
      "ep number 26\n",
      "ep number 27\n",
      "ep number 28\n",
      "ep number 29\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | -44.8    |\n",
      "| steps                   | 1449     |\n",
      "--------------------------------------\n",
      "ep number 30\n",
      "ep number 31\n",
      "ep number 32\n",
      "ep number 33\n",
      "ep number 34\n",
      "ep number 35\n",
      "ep number 36\n",
      "ep number 37\n",
      "ep number 38\n",
      "ep number 39\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | -45.1    |\n",
      "| steps                   | 1949     |\n",
      "--------------------------------------\n",
      "ep number 40\n",
      "ep number 41\n",
      "ep number 42\n",
      "ep number 43\n",
      "ep number 44\n",
      "ep number 45\n",
      "ep number 46\n",
      "ep number 47\n",
      "ep number 48\n",
      "ep number 49\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | -45.3    |\n",
      "| steps                   | 2449     |\n",
      "--------------------------------------\n",
      "ep number 50\n",
      "ep number 51\n",
      "ep number 52\n",
      "ep number 53\n",
      "ep number 54\n",
      "ep number 55\n",
      "ep number 56\n",
      "ep number 57\n",
      "ep number 58\n",
      "ep number 59\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | -45.3    |\n",
      "| steps                   | 2949     |\n",
      "--------------------------------------\n",
      "ep number 60\n",
      "ep number 61\n",
      "ep number 62\n",
      "ep number 63\n",
      "ep number 64\n",
      "ep number 65\n",
      "ep number 66\n",
      "ep number 67\n",
      "ep number 68\n",
      "ep number 69\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | -45.2    |\n",
      "| steps                   | 3449     |\n",
      "--------------------------------------\n",
      "ep number 70\n",
      "ep number 71\n",
      "ep number 72\n",
      "ep number 73\n",
      "ep number 74\n",
      "ep number 75\n",
      "ep number 76\n",
      "ep number 77\n",
      "ep number 78\n",
      "ep number 79\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | -44.9    |\n",
      "| steps                   | 3949     |\n",
      "--------------------------------------\n",
      "ep number 80\n",
      "ep number 81\n",
      "ep number 82\n",
      "ep number 83\n",
      "ep number 84\n",
      "ep number 85\n",
      "ep number 86\n",
      "ep number 87\n",
      "ep number 88\n",
      "ep number 89\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | -45.2    |\n",
      "| steps                   | 4449     |\n",
      "--------------------------------------\n",
      "ep number 90\n",
      "ep number 91\n",
      "ep number 92\n",
      "ep number 93\n",
      "ep number 94\n",
      "ep number 95\n",
      "ep number 96\n",
      "ep number 97\n",
      "ep number 98\n",
      "ep number 99\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -44.9    |\n",
      "| steps                   | 4949     |\n",
      "--------------------------------------\n",
      "ep number 100\n",
      "ep number 101\n",
      "ep number 102\n",
      "ep number 103\n",
      "ep number 104\n",
      "ep number 105\n",
      "ep number 106\n",
      "ep number 107\n",
      "ep number 108\n",
      "ep number 109\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | -45.2    |\n",
      "| steps                   | 5449     |\n",
      "--------------------------------------\n",
      "ep number 110\n",
      "ep number 111\n",
      "ep number 112\n",
      "ep number 113\n",
      "ep number 114\n",
      "ep number 115\n",
      "ep number 116\n",
      "ep number 117\n",
      "ep number 118\n",
      "ep number 119\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | -45      |\n",
      "| steps                   | 5949     |\n",
      "--------------------------------------\n",
      "ep number 120\n",
      "ep number 121\n",
      "ep number 122\n",
      "ep number 123\n",
      "ep number 124\n",
      "ep number 125\n",
      "ep number 126\n",
      "ep number 127\n",
      "ep number 128\n",
      "ep number 129\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | -45.1    |\n",
      "| steps                   | 6449     |\n",
      "--------------------------------------\n",
      "ep number 130\n",
      "ep number 131\n",
      "ep number 132\n",
      "ep number 133\n",
      "ep number 134\n",
      "ep number 135\n",
      "ep number 136\n",
      "ep number 137\n",
      "ep number 138\n",
      "ep number 139\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | -44.9    |\n",
      "| steps                   | 6949     |\n",
      "--------------------------------------\n",
      "ep number 140\n",
      "ep number 141\n",
      "ep number 142\n",
      "ep number 143\n",
      "ep number 144\n",
      "ep number 145\n",
      "ep number 146\n",
      "ep number 147\n",
      "ep number 148\n",
      "ep number 149\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 81       |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | -44.9    |\n",
      "| steps                   | 7449     |\n",
      "--------------------------------------\n",
      "ep number 150\n",
      "ep number 151\n",
      "ep number 152\n",
      "ep number 153\n",
      "ep number 154\n",
      "ep number 155\n",
      "ep number 156\n",
      "ep number 157\n",
      "ep number 158\n",
      "ep number 159\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | -44.8    |\n",
      "| steps                   | 7949     |\n",
      "--------------------------------------\n",
      "ep number 160\n",
      "ep number 161\n",
      "ep number 162\n",
      "ep number 163\n",
      "ep number 164\n",
      "ep number 165\n",
      "ep number 166\n",
      "ep number 167\n",
      "ep number 168\n",
      "ep number 169\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | -45.1    |\n",
      "| steps                   | 8449     |\n",
      "--------------------------------------\n",
      "ep number 170\n",
      "ep number 171\n",
      "ep number 172\n",
      "ep number 173\n",
      "ep number 174\n",
      "ep number 175\n",
      "ep number 176\n",
      "ep number 177\n",
      "ep number 178\n",
      "ep number 179\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 78       |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | -45.3    |\n",
      "| steps                   | 8949     |\n",
      "--------------------------------------\n",
      "ep number 180\n",
      "ep number 181\n",
      "ep number 182\n",
      "ep number 183\n",
      "ep number 184\n",
      "ep number 185\n",
      "ep number 186\n",
      "ep number 187\n",
      "ep number 188\n",
      "ep number 189\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | -45.6    |\n",
      "| steps                   | 9449     |\n",
      "--------------------------------------\n",
      "ep number 190\n",
      "ep number 191\n",
      "ep number 192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep number 193\n",
      "ep number 194\n",
      "ep number 195\n",
      "ep number 196\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barisyazici/anaconda3/envs/sb_action/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Bdq_mountaincar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('Bdq_mountaincar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "#     print(action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "#     print(rewards)\n",
    "    env.render()\n",
    "    if dones:\n",
    "        obs = env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(acts)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_action",
   "language": "python",
   "name": "sb_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
